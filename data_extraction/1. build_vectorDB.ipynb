{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09149776-88f8-46c1-b92b-f21371c87396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pymupdf4llm\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb3eea-8c8c-4dcb-8e58-4aa5edac4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set API Key from OpenAI\n",
    "openai_api_key= \"Add Your OpenAI API KEY Here.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13064c-f305-4e82-8b7e-261f32dff8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_paths(pdf_directory, base_persist_directory):\n",
    "    \"\"\"Load PDF file paths in natural order and assign persist directories in groups of three with local IDs (1,2,3) per batch.\"\"\"    \n",
    "    files = [file for file in os.listdir(pdf_directory) if file.endswith(\".pdf\")]\n",
    "    sorted_files = natsorted(files)  \n",
    "    \n",
    "    all_files = [str(Path(pdf_directory, file).as_posix()) for file in sorted_files]\n",
    "    \n",
    "    num_of_papers = len(all_files)  \n",
    "\n",
    "    document_ids = list(range(1, num_of_papers + 1))\n",
    "\n",
    "    persist_directories = [\n",
    "        str(Path(base_persist_directory, f\"db_{doc_id}\").as_posix())\n",
    "        for doc_id in document_ids\n",
    "    ]\n",
    "    \n",
    "    df_pdf = pd.DataFrame({\n",
    "        'Document_ID': document_ids, \n",
    "        'PDF_path': all_files, \n",
    "        'Vectordb_path': persist_directories\n",
    "    })\n",
    "    \n",
    "    return df_pdf\n",
    "    \n",
    "def load_the_document(pdf_path):\n",
    "    \"\"\"Loads the document and extracts text content\"\"\"\n",
    "    # Load document\n",
    "    docs = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "\n",
    "    # Extract metadata\n",
    "    paper_title = docs[0]['metadata'].get('title', 'Unknown Title')\n",
    "    num_pages = docs[0]['metadata'].get('page_count', 0)\n",
    "    author = docs[0]['metadata'].get('author', 'Unknown Author')\n",
    "\n",
    "    pages_content = []\n",
    "\n",
    "    for i, page in enumerate(docs, start=1):\n",
    "        page_text = remove_reference(page['text']).strip()\n",
    "        pages_content.append({\n",
    "            \"page_number\": i,\n",
    "            \"text\": page_text\n",
    "        })\n",
    "\n",
    "    return pages_content, paper_title, author, num_pages\n",
    "\n",
    "\n",
    "def remove_reference(pdf_text):\n",
    "    \"\"\"Removes references and acknowledgment sections, along with inline citations, from a given PDF text.\"\"\"\n",
    "    # Remove references and acknowledgment sections entirely\n",
    "    pdf_text = re.split(r'(?i)\\bReferences\\b|\\bAcknowledgment[s]?\\b', pdf_text)[0].strip()\n",
    "\n",
    "    # Remove inline citations like [1], [12-15], (Smith et al., 2021), etc.\n",
    "    citation_patterns = [\n",
    "        r'\\[[^\\]]*\\d{1,4}\\]',          # numeric citations like [1], [12], [ABC12]\n",
    "        r'\\([^\\)]*et al\\.,?\\s*\\d{4}[^\\)]*\\)',  # (Author, 2020)\n",
    "        r'\\(\\s*\\d{4}[a-z]?\\s*\\)',  # (2021a)\n",
    "    ]\n",
    "\n",
    "    for pattern in citation_patterns:\n",
    "        pdf_text = re.sub(pattern, '', pdf_text)\n",
    "\n",
    "    # Clean up extra spaces and blank lines\n",
    "    pdf_text = '\\n'.join(line.strip() for line in pdf_text.splitlines() if line.strip())\n",
    "\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "def embedding_document(pages_content, ID, persist_directory):\n",
    "    \"\"\"Embeds documents and stores them persistently in a vector database (Chroma), recording page number.\"\"\"\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,   # small for better retrieval precision\n",
    "        chunk_overlap=400,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\"]\n",
    "    )\n",
    "\n",
    "    document_id = ID\n",
    "\n",
    "    for page in pages_content:\n",
    "        page_number = page[\"page_number\"]\n",
    "        page_text = page[\"text\"]\n",
    "\n",
    "        # Split this page into smaller chunks\n",
    "        page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "        for chunk in page_chunks:\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=chunk.page_content,\n",
    "                    metadata={\n",
    "                        \"page_number\": page_number,\n",
    "                        \"document_id\": document_id \n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "    num_chunks = len(documents)\n",
    "\n",
    "    # Initialize embeddings\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings, \n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    vectordb.persist()\n",
    "\n",
    "    return num_chunks\n",
    "\n",
    "def build_vectorDB(df_path):\n",
    "    \"\"\"Processes each PDF, extracts metadata, builds vector DB, and updates DataFrame.\"\"\"\n",
    "    success_status = []  # Track success for each PDF\n",
    "    paper_titles = []  # Store paper titles\n",
    "    author_list = []  # Store author information\n",
    "    num_pages_list = []  # Store number of pages\n",
    "    chunk_nums = []  # Store chunk numbers separately\n",
    "\n",
    "    for _, row in df_path.iterrows():\n",
    "        pdf_path = row[\"PDF_path\"]\n",
    "        ID = row[\"Document_ID\"]\n",
    "        persist_directory = row[\"Vectordb_path\"]\n",
    "        \n",
    "        try:\n",
    "            # Extract content, title, and page count\n",
    "            docs, paper_title, author, num_pages = load_the_document(pdf_path)\n",
    "\n",
    "            # Build the vector database\n",
    "            num_chunks = embedding_document(docs, ID, persist_directory)  # Returns an integer\n",
    "\n",
    "            success_status.append(True)\n",
    "\n",
    "            # Store extracted metadata\n",
    "            paper_titles.append(paper_title)\n",
    "            author_list.append(author)\n",
    "            num_pages_list.append(num_pages)\n",
    "            chunk_nums.append(num_chunks) \n",
    "\n",
    "            print(f\"Successfully processed {pdf_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "            success_status.append(False)  # Mark as failed\n",
    "\n",
    "            # Store placeholders for failed cases\n",
    "            paper_titles.append(None)\n",
    "            author_list.append(None)\n",
    "            num_pages_list.append(None)\n",
    "            chunk_nums.append(None)  \n",
    "            vectordb = None  # Ensure vectordb is always defined\n",
    "\n",
    "    # Add extracted metadata to the DataFrame\n",
    "    df_path[\"Paper_title\"] = paper_titles\n",
    "    df_path[\"Author\"] = author_list\n",
    "    df_path[\"Num_pages\"] = num_pages_list\n",
    "    df_path[\"Num_chunks\"] = chunk_nums  \n",
    "    df_path[\"Is_added_to_vectorDB\"] = success_status\n",
    "\n",
    "    return df_path\n",
    "\n",
    "def df_to_csv(df, file_name):\n",
    "    \"\"\"Write a DataFrame to a CSV file\"\"\"\n",
    "    df.to_csv(file_name, index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c244e36-5637-4237-8944-3c50d930d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing source PDF files to be processed\n",
    "pdf_directory = \"Add your PDF directory path here\"\n",
    "# Directory where vector databases are persisted\n",
    "base_persist_directory = \"Add your vector DB directory path here\"\n",
    "\n",
    "# Load PDF file paths\n",
    "df_path = load_pdf_paths(pdf_directory, base_persist_directory)\n",
    "# Build the vector database\n",
    "df = build_vectorDB(df_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66071534-513e-4ee2-b2fb-ab2a26bc9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file path for saving the vectordb and metadata\n",
    "file_path = \"Add your output CSV path here\"   # This file will be used in Steps 2 and 3\n",
    "df_to_csv(df, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
