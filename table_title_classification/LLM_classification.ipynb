{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b75d75-32ed-4eec-8e29-27948435025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16905287-8d42-44a5-a79e-87c5470702db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial OpenAI client\n",
    "openai_api_key= \"Add Your OpenAI API KEY Here.\"\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def LCI_title_classification(df):\n",
    "    \"\"\"Classify each table title as 'LCI inventory table' or not (Yes/No)\"\"\"\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        context = row[\"Table_title\"]\n",
    "\n",
    "        # --- System role ---\n",
    "        system_msg = (\n",
    "            \"You are an expert in Life Cycle Assessment (LCA). \"\n",
    "            \"Determine whether the given context represents a life cycle inventory \"\n",
    "            \"(LCI) or input-output table. Respond strictly with only 'Yes' or 'No'.\"\n",
    "        )\n",
    "\n",
    "        # --- Few-shot examples ---\n",
    "        examples = \"\"\"\n",
    "        Context:\n",
    "        Table S9 The inventory data for PV/CCU-CH3OH technical route.\n",
    "        Answer: Yes\n",
    "\n",
    "        Context:\n",
    "        Table S1 details the input-output data of the four ethylene glycol production routes.\n",
    "        Answer: Yes\n",
    "\n",
    "        Context:\n",
    "        Table 1. The plant-level mass and energy balances for manufacturing ethylene.\n",
    "        Answer: Yes\n",
    "\n",
    "        Context:\n",
    "        Table 4 Summary of the main economic indicators of the production process.\n",
    "        Answer: No\n",
    "\n",
    "        Context:\n",
    "        Table 2. Comparison of the environmental impact categories for different production scenarios.\n",
    "        Answer: No\n",
    "\n",
    "        Only report \"Yes\" or \"No\" â€” do not include explanations.\n",
    "        Question: Given the table title and context, is it an LCI inventory table?\n",
    "        \"\"\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "        Answer strictly with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": examples + \"\\n\\n\" + query},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "\n",
    "            if \"yes\" in answer.lower():\n",
    "                answer = \"Yes\"\n",
    "            elif \"no\" in answer.lower():\n",
    "                answer = \"No\"\n",
    "            else:\n",
    "                answer = \"No\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{context[:50]}...': {e}\")\n",
    "            answer = \"No\"\n",
    "\n",
    "        responses.append(answer)\n",
    "\n",
    "    df[\"LLM_prediction\"] = responses\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "\n",
    "# Rename label column\n",
    "df = df.rename(columns={'is LCI inventory table?': 'label'})\n",
    "df['label'] = df['label'].map({'Yes': 'Yes', 'No': 'No'})\n",
    "\n",
    "# Split the dataset for 80% train and 20% test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Run model on test set\n",
    "classified_df = LCI_title_classification(test_df)\n",
    "\n",
    "# Evaluate\n",
    "y_true = classified_df['label']\n",
    "y_pred = classified_df['LLM_prediction']\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-score:  {f1:.3f}\")\n",
    "\n",
    "print(\"\\n=== Detailed Report ===\")\n",
    "print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d1957-aeed-4c93-9a21-e766937b0c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
